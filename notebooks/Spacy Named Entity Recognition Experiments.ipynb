{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regional-orbit",
   "metadata": {},
   "source": [
    "# Spacy Named Entity Recognition Experiments\n",
    "This notebook is for practice with Spacy's named entity recognition.  The aim being to formulate the process for taking the text created from the PDF and recognizing the required features to extract and save in the Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-advantage",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "smooth-probe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: setuptools in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jeremyspradlin/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-Kg_LyPhr/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "native-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Doc Analysis\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Imports for uploading PDF and converting it to text\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path, convert_from_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "unknown-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    \"\"\"\n",
    "    Takes the path to a PDF file, creates a PIL image, and then reads through the image\n",
    "    convertin the images into text.\n",
    "    \n",
    "    INPUT: path\n",
    "    \n",
    "    RETURNS: Text Object\n",
    "    \"\"\"\n",
    "    fulltext = ''\n",
    "    \n",
    "    # Read in the pdf to the PIL image\n",
    "    pil_image = convert_from_path(path)\n",
    "    \n",
    "    # Iterate through PIL image and convert each page to text\n",
    "    text = [str(pytesseract.image_to_string(image)) for image in pil_image]\n",
    "\n",
    "    # Iterate through the raw text and format returns\n",
    "    for t in text:\n",
    "        fulltext += t\n",
    "        \n",
    "    return fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tested-annual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-sharp",
   "metadata": {},
   "source": [
    "### Extracting basic entities\n",
    "The function below will start to extract entities out of PDF files whose path is fed to it.  The goal for right now is to streamline the process from path to entities in order to make comparisons between different pdf entities to look for commonalities in trying to extract relevant features with a high degree of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "sunset-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(pdf_path):\n",
    "    \"\"\"\n",
    "    Function that takes in a path to a pdf file, uses the get_text() function to convert \n",
    "    the file to text, and then uses SpaCy to extract and return relevant entities.\n",
    "    \n",
    "    INPUT: path to pdf file\n",
    "    \n",
    "    RETURNS: list of people, orgs, and other entities as relevant for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the pdf file to text, and then fit it to the NLP Model\n",
    "    text = get_text(pdf_path)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Setup lists to append entities to\n",
    "    people = set()\n",
    "    orgs = set()\n",
    "    norps = set()\n",
    "    gpes = set()\n",
    "    \n",
    "    # Iterate through the entities, compare for relevant entities, and append them to \n",
    "    # the relevant list\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # print('Name: ', ent.text)  # Printing lines for pattern testing\n",
    "            people.add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            # print('ORG: ', ent.text)\n",
    "            orgs.add(ent.text)\n",
    "        elif ent.label_ == \"NORP\":\n",
    "            # print('NORP: ', ent.text)\n",
    "            norps.add(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            # print('GPE: ', ent.text)\n",
    "            gpes.add(ent.text)\n",
    "            \n",
    "    # Return the entity lists\n",
    "    print('done')\n",
    "    return people, orgs, norps, gpes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "alone-population",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "particular-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Virginia', 'U.S.', 'the United\\n\\nStates', 'Falls Church', 'MD 21201', 'Maryland', 'the United States', 'Mexico', 'Baltimore'}\n",
      "{'Lopez-Mendoza', 'Jennifer Piateski', 'File Nos', 'Jennifer BE', 'Denna Cane', 'joWOeIT MMM', 'Donna Carr', 'Charles K.\\n', 'John', 'Sweeney', 'DAVID W. CROSLAND', 'Maureen A.', 'Maureen A. Sweeney', 'Ferino Sanchez Seltik'}\n",
      "{'Court', 'Cite', 'Office of the Clerk', 'U.S. Department of Justice', 'Sony', 'Matter of Toro', 'the Executive Office', 'Executive Office for Immigration Review\\n\\nBoard of Immigration Appeals', 'Section C', 'the Executive Office for Immigration Review', 'an Appellate Court', 'Government', 'Free State Reporting, Inc.', 'the Immigration Court', 'United States Immigration', 'Leesburg Pike', 'the\\n\\nImmigration and Nationality Act', 'the Supreme Court', 'FERINO', 'Executive Office for Immigration Review\\n\\n', 'Board', 'The Board', 'DHS/ICE Office of Chief Counsel', 'I&N', 'DAVILA', 'Contractor', 'OARD', 'Esquire\\n\\nUniversity of Maryland Immigration Clinic', 'the Board of Immigration Appeals', 'Immigration Judges de', 'U.S. Department of Justice Decision'}\n",
      "{'Cuban', 'FERINO', 'CUBAN'}\n"
     ]
    }
   ],
   "source": [
    "print(gpes)\n",
    "print(people)\n",
    "print(orgs)\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "inside-truck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('appeal_test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "jewish-norman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Virginia', 'Northpoint Drive', 'Copan', 'Falls Church', 'Honduras', 'TX', 'the United States', 'remand', 'Guatemala', 'Houston'}\n",
      "{'Sheridan Gary DHSI/ICE Office', 'Sheridan Green Law PLLC', 'Esquire\\n\\n', 'AXXX XXX 957', 'Honduras', 'Nimmo Bhagat', 'Chen', 'QJ', 'Gavino Pineda', 'Cynthia L. Crosby', 'Sheridan G. Green', 'Tab D', 'ANALYSIS', 'Tortus', 'Maura Suyapa Varela-Erazo', 'Gonzales', 'Jose', 'Keily Janeth'}\n",
      "{'XZ', 'The Fifth Circuit', 'Court', 'the Country Report', 'the U.S. Department of State', '|-589', 'Board of Immigration Appeals', 'Whenthe-Court', 'IJ', 'Torture', 'Department', 'Office of the Clerk EJ', 'AXXX XXX 957', 'INA Section', 'Executive Office for Immigration Review', 'Board', 'DHS', 'U.S. Department of Justice\\n\\nExecutive Office for Immigration Review\\n\\n \\n\\n', 'I&N', 'the Board of Immigration Appeals', 'GE', 'Bi Department', 'CREDIBILITY\\n\\n', 'U.S. Department of Justice Decision', 'The Department of Homeland Security', 'United', 'Homeland Security', 'the United Nations'}\n",
      "{'Guatemalan', 'ty'}\n"
     ]
    }
   ],
   "source": [
    "print(gpes)\n",
    "print(people)\n",
    "print(orgs)\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "activated-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('test1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ready-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Virginia', 'MA', 'Falls Church', 'Denna', 'Florida', 'Boston'}\n",
      "{'P.O. Box 8728', 'LAUDELINO', 'Gwendylan Tregerman', 'Miller', 'Mark D. Cooper', 'Donna Carr', 'Esq', 'Mark D.', 'Neil P.\\n\\n'}\n",
      "{'DHS', 'DHS/ICE Office of Chief Counsel', 'JOAO SILVA LAUDELINO', 'JOAO', 'Cite', 'Leesburg Pike', 'Executive Office for Immigration Review\\n\\nBoard of Immigration Appeals', 'Enclosure\\n\\n', 'the Board of Immigration Appeals', 'Falls Church', 'SNjay', 'Office of the Clerk\\n\\n \\n\\nCooper', 'U.S. Department of Justice', 'U.S. Department of Justice Decision', 'Executive Office for Immigration Review', 'The Department of Homeland Security', 'Board', 'Joao Silva Laudelino'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(gpes)\n",
    "print(people)\n",
    "print(orgs)\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-blade",
   "metadata": {},
   "source": [
    "### Refining the entities to extract specific information\n",
    "We now have a straightforward process for extracting out lists of entities from a path to a pdf file.  However, we have the issue of getting many entities that are similar in nature, and we need to determine heuristics for determining which specific entities match for feature extraction. (e.g. We can pull out several names, but need a workable heuristic to reliably determine who is who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
